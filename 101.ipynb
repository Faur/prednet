{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PredNet\n",
    "\n",
    "Train PredNet on KITTI sequences. (Geiger et al. 2013, http://www.cvlibs.net/datasets/kitti/)\n",
    "\n",
    "* Python 3.5.2\n",
    "* tensorflow-gpu 0.12.1\n",
    "* Keras 1.0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from six.moves import cPickle\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from prednet import PredNet\n",
    "from data_utils import SequenceGenerator\n",
    "from kitti_settings import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model files\n",
    "save_model = True\n",
    "weights_file = os.path.join(WEIGHTS_DIR, 'toke_weights.hdf5')\n",
    "json_file = os.path.join(WEIGHTS_DIR, 'toke_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Data files\n",
    "train_file = os.path.join(DATA_DIR, 'X_train.hkl')\n",
    "train_sources = os.path.join(DATA_DIR, 'sources_train.hkl')\n",
    "val_file = os.path.join(DATA_DIR, 'X_val.hkl')\n",
    "val_sources = os.path.join(DATA_DIR, 'sources_val.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training parameters\n",
    "nb_epoch = 150\n",
    "batch_size = 4\n",
    "samples_per_epoch = 500\n",
    "N_seq_val = 100  # number of sequences to use for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model parameters\n",
    "nt = 10 # Number of timesteps per sequence\n",
    "n_channels, im_height, im_width = (3, 128, 160) #input shape\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    input_shape = (n_channels, im_height, im_width)\n",
    "else:\n",
    "    input_shape = (im_height, im_width, n_channels)\n",
    "stack_sizes = (n_channels, 48, 96, 192) #feature map sizes\n",
    "R_stack_sizes = stack_sizes\n",
    "\n",
    "A_filt_sizes = (3, 3, 3)\n",
    "Ahat_filt_sizes = (3, 3, 3, 3)\n",
    "R_filt_sizes = (3, 3, 3, 3)\n",
    "layer_loss_weights = np.array([1., 0.1, 0.1, 0.1])\n",
    "layer_loss_weights = np.expand_dims(layer_loss_weights, 1)\n",
    "time_loss_weights = 1./ (nt - 1) * np.ones((nt,1))\n",
    "time_loss_weights[0] = 0 # Don't use loss on initial timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating the model\n",
    "prednet = PredNet(stack_sizes, # Number of channels in Conv layers (A, Ahat)\n",
    "                  R_stack_sizes, # Number of channels in Conv LSTM layers (R)\n",
    "                  A_filt_sizes, # Filter size\n",
    "                  Ahat_filt_sizes, \n",
    "                  R_filt_sizes,\n",
    "                  output_mode='error', \n",
    "                  return_sequences=True)\n",
    "\n",
    "inputs = Input(shape=(nt,) + input_shape)\n",
    "errors = prednet(inputs)  # errors will be (batch_size, nt, nb_layers)\n",
    "# errors_by_time = TimeDistributed(\n",
    "#     Dense(\n",
    "#         1,\n",
    "#         weights=[layer_loss_weights, np.zeros(1)], \n",
    "#         trainable=False), \n",
    "#     trainable=False)(errors)  # calculate weighted error by layer\n",
    "\n",
    "# errors_by_time = Flatten()(errors_by_time)  # will be (batch_size, nt)\n",
    "# final_errors = Dense(1,\n",
    "#     weights=[time_loss_weights, np.zeros(1)],\n",
    "#     trainable=False)(errors_by_time)  # weight errors by time\n",
    "# model = Model(input=inputs, output=final_errors)\n",
    "# model.compile(loss='mean_absolute_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
